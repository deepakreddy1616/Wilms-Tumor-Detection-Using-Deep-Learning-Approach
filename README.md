# ğŸ¯ Wilms Tumor Detection using Deep Learning Approach

[![Python](https://img.shields.io/badge/Python-3.10+-blue?style=flat-square&logo=python)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.3.0-red?style=flat-square&logo=pytorch)](https://pytorch.org/)
[![YOLOv8](https://img.shields.io/badge/YOLOv8-8.2+-green?style=flat-square&logo=ultralytics)](https://github.com/ultralytics/ultralytics)
[![License](https://img.shields.io/badge/License-MIT-yellow?style=flat-square)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Active-brightgreen?style=flat-square)](https://github.com)
---

## ğŸ¯ Overview

A **deep learning-based automated detection system** for Wilms tumors in medical images using the **YOLOv8 object detection** model. This project implements an end-to-end pipeline including data augmentation, model training, evaluation, and visualization.

**Why this matters:** Wilms tumor is the most common renal malignancy in children. Early and accurate detection is crucial for better treatment outcomes. This system aims to assist radiologists in faster and more accurate diagnosis.

### Problem Statement
- Manual identification of tumors is time-consuming
- Prone to human error and variability
- Requires experienced radiologists
- Need for automated, consistent detection system

### Solution
Implemented a YOLOv8-based object detection model with comprehensive data augmentation and evaluation metrics.

---

## ğŸ“Š Key Results

| Metric | Value | Status |
|--------|-------|--------|
| **mAP@50** | 79.6% | âœ… Excellent |
| **mAP@75** | 65.2% | âœ… Good |
| **Precision** | 69.2% | âœ… High |
| **Recall** | 74.2% | âœ… High |
| **F1-Score** | 0.717 | âœ… Strong |
| **Classes Detected** | 2 (Wilms, Other Tumors) | âœ… Multi-class |
| **Training Time** | ~2 hours (GPU) | âš¡ Fast |
| **Inference Time** | ~50ms/image | ğŸš€ Real-time |

---

## âœ¨ Features

âœ… **Data Augmentation Pipeline**
   - 16x augmentation using Albumentations
   - Horizontal/Vertical flips, rotations, elastic transforms
   - Brightness/contrast adjustments
   - Optical and grid distortions

âœ… **YOLOv8 Implementation**
   - Pre-trained weights from Ultralytics
   - Fine-tuned for medical imaging
   - Multi-scale feature extraction
   - Real-time inference capability

âœ… **Comprehensive Evaluation**
   - mAP@50, mAP@75, mAP@95 metrics
   - Precision, Recall, F1-Score calculations
   - Confusion matrices and ROC curves
   - Per-class performance analysis

âœ… **Visualization Tools**
   - Training curves (loss, mAP, precision)
   - Detection result visualization
   - Model architecture diagrams
   - Inference examples

âœ… **Easy-to-Use Interface**
   - Command-line arguments for flexibility
   - Configurable parameters
   - Logging and progress tracking

---

## ğŸ› ï¸ Technology Stack

| Component | Technology |
|-----------|-----------|
| **Deep Learning Framework** | PyTorch 2.3.0+ |
| **Object Detection** | YOLOv8 (Ultralytics) |
| **Image Processing** | OpenCV, PIL |
| **Data Augmentation** | Albumentations |
| **Scientific Computing** | NumPy, Pandas |
| **Visualization** | Matplotlib, Seaborn |
| **Language** | Python 3.10+ |
| **Hardware** | CUDA-capable GPU (recommended) |

---

## ğŸš€ Quick Start

### Prerequisites

```bash
- Python 3.10 or higher
- pip (Python package manager)
- Git
- CUDA Toolkit 11.8+ (for GPU acceleration - recommended)
- CUDNN (for GPU support)
```

### Installation

1. **Clone the repository:**
```bash
git clone https://github.com/YOUR_USERNAME/Wilms-Tumor-Detection-Using-Deep-Learning-Approach.git
cd Wilms-Tumor-Detection-Using-Deep-Learning-Approach
```

2. **Create Virtual Environment:**
```bash
# Using venv
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Activate (Mac/Linux)
source venv/bin/activate
```

3. **Install Dependencies:**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

4. **Verify Installation:**
```bash
python -c "import torch; print(f'PyTorch version: {torch.__version__}')"
python -c "from ultralytics import YOLO; print('YOLOv8 ready')"
```

### Basic Usage

```bash
# Data Augmentation
python wilmstumordetection.py --augment \
    --input_images data/images \
    --input_labels data/labels \
    --output_dir data/augmented

# Model Training
python wilmstumordetection.py --train \
    --epochs 80 \
    --batch_size 16 \
    --lr 0.001

# Full Pipeline (Augment + Train)
python wilmstumordetection.py --augment --train \
    --epochs 80 \
    --augmentation_factor 16

# Get Help
python wilmstumordetection.py --help
```

---

## ğŸ“ Project Structure

```
Wilms-Tumor-Detection-Using-Deep-Learning-Approach/
â”‚
â”œâ”€â”€ wilmstumordetection.py          # Main training script
â”œâ”€â”€ wilmstumordetection.ipynb       # Jupyter notebook version
â”œâ”€â”€ project-paper.pdf               # Detailed methodology & research
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ LICENSE                          # MIT License
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .gitignore                       # Git ignore rules
â”‚
â”œâ”€â”€ data/                            # Dataset directory
â”‚   â”œâ”€â”€ images/                      # Original medical images
â”‚   â”œâ”€â”€ labels/                      # Annotations (YOLO format)
â”‚   â”œâ”€â”€ augmented/                   # Augmented images (generated)
â”‚   â””â”€â”€ yolov8dataset/               # YOLOv8 dataset format (generated)
â”‚
â”œâ”€â”€ runs/                            # YOLOv8 training outputs
â”‚   â”œâ”€â”€ detect/                      # Detection results
â”‚   â”œâ”€â”€ train/                       # Training runs
â”‚   â””â”€â”€ val/                         # Validation results
â”‚
â””â”€â”€ outputs/                         # Model outputs
    â”œâ”€â”€ augmented_images/            # Augmented dataset
    â”œâ”€â”€ metrics/                     # Performance metrics
    â””â”€â”€ visualizations/              # Graphs and charts
```

---

## ğŸ“Š Results & Metrics

### Training Performance

```
Epoch 1-10:    mAP increasing from 30% to 65%
Epoch 10-30:   Rapid improvement to 75%
Epoch 30-60:   Fine-tuning phase, mAP reaches 78%
Epoch 60-80:   Convergence, final mAP@50 = 79.6%
```

### Confusion Matrix
- **True Positives (TP)**: 185/249 (74.2%)
- **False Positives (FP)**: High precision minimizes false alarms
- **False Negatives (FN)**: Low miss rate ensures tumor detection

### Class-wise Performance
| Class | Precision | Recall | F1-Score | Count |
|-------|-----------|--------|----------|-------|
| Wilms Tumor | 71.2% | 76.8% | 0.74 | 2,150 |
| Other Tumors | 67.1% | 71.5% | 0.69 | 1,840 |
| **Overall** | **69.2%** | **74.2%** | **0.72** | 3,990 |

---

## ğŸ§  How It Works

### 1. Data Augmentation Phase
```
Original Images (100)
        â†“
Albumentations Pipeline
        â†“
16x Augmentation Factor
        â†“
Augmented Dataset (1,600)
```

**Augmentation Techniques:**
- Geometric: Rotations, flips, elastic transforms
- Photometric: Brightness, contrast, blur
- Spatial: Shift, scale, optical distortion

### 2. Model Architecture
```
YOLOv8 Architecture:
Input Image (640Ã—640)
        â†“
Backbone (CSPDarknet)
        â†“
Neck (PAN)
        â†“
Head (Detection layers)
        â†“
Bounding Boxes + Confidence Scores
```

### 3. Training Pipeline
```
Data Loading
    â†“
Model Initialization (YOLOv8m)
    â†“
Loss Calculation (Focal Loss)
    â†“
Backpropagation
    â†“
Parameter Updates (SGD Optimizer)
    â†“
Validation & Metrics
    â†“
Model Checkpointing
```

### 4. Inference Process
```
Medical Image Input
    â†“
Preprocessing & Normalization
    â†“
YOLOv8 Model Forward Pass
    â†“
NMS (Non-Maximum Suppression)
    â†“
Bounding Box Predictions
    â†“
Confidence Visualization
```

---

## ğŸ“ˆ Visualizations

The project generates:
- **Training curves** (Loss, mAP, Precision, Recall)
- **Confusion matrices** for each class
- **Precision-Recall curves** (PR curves)
- **Detection examples** with bounding boxes
- **Feature maps** from different layers

---

## ğŸ” Key Implementation Details

### Data Format (YOLO)
```
images/
â”œâ”€â”€ image1.jpg
â”œâ”€â”€ image2.jpg
â””â”€â”€ ...

labels/
â”œâ”€â”€ image1.txt  # Format: <class_id> <x_center> <y_center> <width> <height>
â”œâ”€â”€ image2.txt
â””â”€â”€ ...
```

### Training Configuration
```python
Model: YOLOv8 Medium (m) variant
Epochs: 80
Batch Size: 16
Learning Rate: 0.001
Optimizer: SGD with momentum=0.937
Loss Function: YOLOv8 Focal Loss (weighted)
Augmentation: Albumentations (16x)
```

---

## ğŸ’¡ Future Improvements

- [ ] Deploy as web API (Flask/FastAPI)
- [ ] Create interactive Streamlit demo
- [ ] Add real-time video inference
- [ ] Mobile app development (TensorFlow Lite)
- [ ] Multi-modal integration (CT scans + MRI)
- [ ] Explainability features (Grad-CAM, attention maps)
- [ ] Benchmark against other models (Faster RCNN, EfficientDet)
- [ ] Publish to Hugging Face Model Hub
- [ ] Create Docker container for easy deployment
- [ ] Add confidence calibration techniques

---

## ğŸ“š Documentation

For detailed methodology, experimental results, and ablation studies, see:
ğŸ“„ **[project-paper.pdf](project-paper.pdf)**

---

## ğŸ”— Related Resources

- [YOLOv8 Documentation](https://docs.ultralytics.com/)
- [PyTorch Documentation](https://pytorch.org/docs/)
- [Albumentations](https://albumentations.ai/)
- [Medical Imaging in Deep Learning](https://arxiv.org/list/eess.IV/recent)

---

*Built with â¤ï¸ using Python, PyTorch, and YOLOv8*
